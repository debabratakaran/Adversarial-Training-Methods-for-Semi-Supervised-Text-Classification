{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from time import sleep\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras as K\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = np.load(\"./dataset/imdb/nltk_dictionary.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = [f for f in listdir(\"./dataset/imdb/train/pos\") if ( isfile(join(\"./dataset/imdb/train/pos\", f)) and (f[0] != '.') ) ]\n",
    "neg_files = [f for f in listdir(\"./dataset/imdb/train/neg\") if ( isfile(join(\"./dataset/imdb/train/neg\", f)) and (f[0] != '.') ) ]\n",
    "unl_files = [f for f in listdir(\"./dataset/imdb/train/unsup\") if ( isfile(join(\"./dataset/imdb/train/unsup\", f)) and (f[0] != '.') ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( \"Labeled...\" )\n",
    "xtrain = list()\n",
    "ytrain = list()\n",
    "for p in pos_files:\n",
    "    pos = open(\"./dataset/imdb/train/pos/\"+p, 'r')\n",
    "    for line in pos:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            tokens = tokenizer.tokenize(line.lower())\n",
    "            tokens = [ (dictionary[t] if (t in dictionary) else dictionary['<unk>']) for t in tokens ] \n",
    "            \n",
    "            xtrain.append(tokens)\n",
    "            ytrain.append(1)     \n",
    "        else:\n",
    "            print(\"empty line: {}.\".format(line))\n",
    "    pos.close()\n",
    "    \n",
    "for n in neg_files:\n",
    "    neg = open(\"./dataset/imdb/train/neg/\"+n, 'r')\n",
    "    for line in neg:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            tokens = tokenizer.tokenize(line.lower())\n",
    "            tokens = [ (dictionary[t] if (t in dictionary) else dictionary['<unk>']) for t in tokens ] \n",
    "            \n",
    "            xtrain.append(tokens)\n",
    "            ytrain.append(0)       \n",
    "        else:\n",
    "            print(\"empty line: {}.\".format(line))\n",
    "    neg.close()\n",
    "\n",
    "print( \"Unlabeled...\" )\n",
    "unlab = list()\n",
    "for u in unl_files:\n",
    "    unl = open(\"./dataset/imdb/train/unsup/\"+u, 'r')\n",
    "    for line in unl:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            tokens = tokenizer.tokenize(line.lower())\n",
    "            tokens = [ (dictionary[t] if (t in dictionary) else dictionary['<unk>']) for t in tokens ] \n",
    "            \n",
    "            unlab.append(tokens)            \n",
    "        else:\n",
    "            print(\"empty line: {}.\".format(line))\n",
    "    unl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = np.asarray(xtrain)\n",
    "ytrain = np.asarray(ytrain)\n",
    "unlab = np.asarray(unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./dataset/imdb/nltk_xtrain.npy\", xtrain)\n",
    "np.save(\"./dataset/imdb/nltk_ytrain.npy\", ytrain)\n",
    "np.save(\"./dataset/imdb/nltk_ultrain.npy\", unlab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = [f for f in listdir(\"./dataset/imdb/test/pos\") if ( isfile(join(\"./dataset/imdb/test/pos\", f)) and (f[0] != '.') ) ]\n",
    "neg_files = [f for f in listdir(\"./dataset/imdb/test/neg\") if ( isfile(join(\"./dataset/imdb/test/neg\", f)) and (f[0] != '.') ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = list()\n",
    "ytest = list()\n",
    "for p in pos_files:\n",
    "    pos = open(\"./dataset/imdb/test/pos/\"+p, 'r')\n",
    "    for line in pos:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            tokens = tokenizer.tokenize(line.lower())\n",
    "            tokens = [ (dictionary[t] if (t in dictionary) else dictionary['<unk>']) for t in tokens ] \n",
    "            \n",
    "            xtest.append(tokens)\n",
    "            ytest.append(1)\n",
    "                \n",
    "        else:\n",
    "            print(\"empty line: {}.\".format(line))\n",
    "    pos.close()\n",
    "    \n",
    "for n in neg_files:\n",
    "    neg = open(\"./dataset/imdb/test/neg/\"+n, 'r')\n",
    "    for line in neg:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            tokens = tokenizer.tokenize(line.lower())\n",
    "            tokens = [ (dictionary[t] if (t in dictionary) else dictionary['<unk>']) for t in tokens ] \n",
    "            \n",
    "            xtest.append(tokens)\n",
    "            ytest.append(0)\n",
    "                \n",
    "        else:\n",
    "            print(\"empty line: {}.\".format(line))\n",
    "    neg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.asarray(xtest)\n",
    "ytest = np.asarray(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./dataset/imdb/nltk_xtest.npy\", xtest)\n",
    "np.save(\"./dataset/imdb/nltk_ytest.npy\", ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max( [len(s) for s in x] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = \"./dataset/imdb/train/\"\n",
    "merged = open(\"./merged.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = old_path + 'pos'\n",
    "files = [f for f in listdir(current_directory) if ( isfile(join(current_directory, f)) and (f[0] != '.') ) ]\n",
    "for f in files:\n",
    "    t = open(current_directory+'/'+f, 'r')\n",
    "    for line in t:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            merged.write(line+'\\n')\n",
    "    t.close()\n",
    "\n",
    "current_directory = old_path + 'neg'\n",
    "files = [f for f in listdir(current_directory) if ( isfile(join(current_directory, f)) and (f[0] != '.') ) ]\n",
    "for f in files:\n",
    "    t = open(current_directory+'/'+f, 'r')\n",
    "    for line in t:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            merged.write(line+'\\n')\n",
    "    t.close()\n",
    "\n",
    "current_directory = old_path + 'unsup'\n",
    "files = [f for f in listdir(current_directory) if ( isfile(join(current_directory, f)) and (f[0] != '.') ) ]\n",
    "for f in files:\n",
    "    t = open(current_directory+'/'+f, 'r')\n",
    "    for line in t:\n",
    "        if line:\n",
    "            line = line.replace('<br /><br />', ' ')\n",
    "            merged.write(line+'\\n')\n",
    "    t.close()\n",
    "\n",
    "merged.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"./merged.txt\", \"r\")\n",
    "new_corpus = open(\"./GloVe-1.2/new_merged\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = corpus.readlines()\n",
    "text = ''\n",
    "\n",
    "for line in lines:\n",
    "    text = text + line\n",
    "corpus.close()\n",
    "\n",
    "words = tokenizer.tokenize(text.lower())\n",
    "\n",
    "for w in words:\n",
    "    new_corpus.seek(0, 2)\n",
    "    new_corpus.write(w + ' ')\n",
    "new_corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = dict()\n",
    "dictionary = dict()\n",
    "vectors = open(\"./GloVe-1.2/vectors.txt\", \"r\")\n",
    "for i, line in enumerate(vectors):\n",
    "    sline = line.split(' ')\n",
    "    try:\n",
    "        word = sline[0]\n",
    "        word_embedding[word] = np.asarray(sline[1:], dtype='float32')\n",
    "        dictionary[word] = i+1\n",
    "    except:\n",
    "        print(\"error at index {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.zeros( shape=(len(dictionary.keys())+1, len(list(word_embedding.values())[0])))\n",
    "for word in dictionary.keys():\n",
    "    idx = dictionary[word]\n",
    "    embedding[idx] = word_embedding[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./dataset/imdb/nltk_embedding_matrix.npy\", embedding)\n",
    "np.save(\"./dataset/imdb/nltk_dictionary.npy\", dictionary)\n",
    "np.save(\"./dataset/imdb/nltk_word_embedding.npy\", word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, session, dict_weight, dropout=0.2, lstm_units=1024, dense_units=30):\n",
    "        self.sess = session\n",
    "        K.backend.set_session(self.sess)\n",
    "        #defining layers\n",
    "        dict_shape = dict_weight.shape\n",
    "        self.emb = K.layers.Embedding(dict_shape[0], dict_shape[1], weights=[dict_weight], trainable=False, name='embedding')\n",
    "        self.drop = K.layers.Dropout(rate=dropout, seed=91, name='dropout')\n",
    "        self.lstm = K.layers.LSTM(lstm_units, stateful=False, return_sequences=False, name='lstm')\n",
    "        self.dense = K.layers.Dense(dense_units, activation='relu', name='dense')\n",
    "        self.p = K.layers.Dense(1, activation='sigmoid', name='p')\n",
    "        #defining optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "        # self.optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "    def __call__(self, batch, perturbation=None):\n",
    "        embedding = self.emb(batch) \n",
    "        drop = self.drop(embedding)\n",
    "        if (perturbation is not None):\n",
    "            drop += perturbation\n",
    "        lstm = self.lstm(drop)\n",
    "        dense = self.dense(lstm)\n",
    "        return self.p(dense), embedding\n",
    "    \n",
    "    def get_minibatch(self, x, y, ul, batch_shape=(64, 400)):\n",
    "        x = K.preprocessing.sequence.pad_sequences(x, maxlen=batch_shape[1])\n",
    "        permutations = np.random.permutation( len(y) )\n",
    "        ul_permutations = None\n",
    "        len_ratio = None\n",
    "        if (ul is not None):\n",
    "            ul = K.preprocessing.sequence.pad_sequences(ul, maxlen=batch_shape[1])\n",
    "            ul_permutations = np.random.permutation( len(ul) )\n",
    "            len_ratio = len(ul)/len(y)\n",
    "        for s in range(0, len(y), batch_shape[0]):\n",
    "            perm = permutations[s:s+batch_shape[0]]\n",
    "            minibatch = {'x': x[perm], 'y': y[perm]}\n",
    "            if (ul is not None):\n",
    "                ul_perm = ul_permutations[int(np.floor(len_ratio*s)):int(np.floor(len_ratio*(s+batch_shape[0])))]\n",
    "                minibatch.update( {'ul': np.concatenate((ul[ul_perm], x[perm]), axis=0)} )\n",
    "            yield minibatch\n",
    "            \n",
    "    def get_loss(self, batch, labels):\n",
    "        pred, emb = self(batch)\n",
    "        loss = K.losses.binary_crossentropy(labels, pred)\n",
    "        return tf.reduce_mean( loss ), emb\n",
    "    \n",
    "    def get_adv_loss(self, batch, labels, loss, emb, p_mult):\n",
    "        gradient = tf.gradients(loss, emb, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "        p_adv = p_mult * tf.nn.l2_normalize(tf.stop_gradient(gradient), dim=1)\n",
    "        adv_loss = K.losses.binary_crossentropy(labels, self(batch, p_adv)[0])\n",
    "        return tf.reduce_mean( adv_loss )\n",
    "    \n",
    "    def get_v_adv_loss(self, ul_batch, p_mult, power_iterations=1):\n",
    "        bernoulli = tf.distributions.Bernoulli\n",
    "        prob, emb = self(ul_batch)\n",
    "        prob = tf.clip_by_value(prob, 1e-7, 1.-1e-7)\n",
    "        prob_dist = bernoulli(probs=prob)\n",
    "        #generate virtual adversarial perturbation\n",
    "        d = tf.random_uniform(shape=tf.shape(emb), dtype=tf.float32)\n",
    "        for _ in range( power_iterations ):\n",
    "            d = (0.02) * tf.nn.l2_normalize(d, dim=1)\n",
    "            p_prob = tf.clip_by_value(self(ul_batch, d)[0], 1e-7, 1.-1e-7)\n",
    "            kl = tf.distributions.kl_divergence(prob_dist, bernoulli(probs=p_prob), allow_nan_stats=False)\n",
    "            gradient = tf.gradients(kl, [d], aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "            d = tf.stop_gradient(gradient)\n",
    "        d = p_mult * tf.nn.l2_normalize(d, dim=1)\n",
    "        tf.stop_gradient(prob)\n",
    "        #virtual adversarial loss\n",
    "        p_prob = tf.clip_by_value(self(ul_batch, d)[0], 1e-7, 1.-1e-7)\n",
    "        v_adv_loss = tf.distributions.kl_divergence(prob_dist, bernoulli(probs=p_prob), allow_nan_stats=False)\n",
    "        return tf.reduce_mean( v_adv_loss )\n",
    "\n",
    "    def validation(self, x, y, batch_shape=(64, 400)):\n",
    "        print( 'Validation...' )\n",
    "        \n",
    "        labels = tf.placeholder(tf.float32, shape=(None, 1), name='validation_labels')\n",
    "        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='validation_batch')\n",
    "\n",
    "        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n",
    "        \n",
    "        accuracies = list()\n",
    "        minibatch = self.get_minibatch(x, y, ul=None, batch_shape=batch_shape)\n",
    "        for val_batch in minibatch:\n",
    "            fd = {batch: val_batch['x'], labels: val_batch['y'], K.backend.learning_phase(): 0} #test mode\n",
    "            accuracies.append( self.sess.run(accuracy, feed_dict=fd) )\n",
    "        \n",
    "        print( \"Average accuracy on validation is {:.3f}\".format(np.asarray(accuracies).mean()) )\n",
    "    \n",
    "    def train(self, dataset, batch_shape=(64, 400), epochs=10, loss_type='none', p_mult=0.02, init=None, save=None):\n",
    "        print( 'Training...' )\n",
    "        xtrain = np.load( \"{}nltk_xtrain.npy\".format(dataset) )\n",
    "        ytrain = np.load( \"{}nltk_ytrain.npy\".format(dataset) )\n",
    "        ultrain = np.load( \"{}nltk_ultrain.npy\".format(dataset) ) if (loss_type == 'v_adv') else None\n",
    "        \n",
    "        # defining validation set\n",
    "        xval = list()\n",
    "        yval = list()\n",
    "        for _ in range( int(len(ytrain)*0.025) ):\n",
    "            xval.append( xtrain[0] ); xval.append( xtrain[-1] )\n",
    "            yval.append( ytrain[0] ); yval.append( ytrain[-1] )\n",
    "            xtrain = np.delete(xtrain, 0); xtrain = np.delete(xtrain, -1)\n",
    "            ytrain = np.delete(ytrain, 0); ytrain = np.delete(ytrain, -1)\n",
    "        xval = np.asarray(xval)\n",
    "        yval = np.asarray(yval)\n",
    "        print( '{} elements in validation set'.format(len(yval)) )\n",
    "        # ---\n",
    "        yval = np.reshape(yval, newshape=(yval.shape[0], 1))\n",
    "        ytrain = np.reshape(ytrain, newshape=(ytrain.shape[0], 1))\n",
    "        \n",
    "        labels = tf.placeholder(tf.float32, shape=(None, 1), name='train_labels')\n",
    "        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='train_batch')\n",
    "        ul_batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='ul_batch')\n",
    "        \n",
    "        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n",
    "        loss, emb = self.get_loss(batch, labels)\n",
    "        if (loss_type == 'adv'):\n",
    "            loss += self.get_adv_loss(batch, labels, loss, emb, p_mult)\n",
    "        elif (loss_type == 'v_adv'):\n",
    "            loss += self.get_v_adv_loss(ul_batch, p_mult)\n",
    "\n",
    "        opt = self.optimizer.minimize( loss )\n",
    "        #initializing parameters\n",
    "        if (init is None):\n",
    "            self.sess.run( [var.initializer for var in tf.global_variables() if not('embedding' in var.name)] )\n",
    "            print( 'Random initialization' )\n",
    "        else:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self.sess, init)\n",
    "            print( 'Restored value' )\n",
    "        \n",
    "        _losses = list()\n",
    "        _accuracies = list()\n",
    "        list_ratio = (len(ultrain)/len(ytrain)) if (ultrain is not None) else None\n",
    "        for epoch in range(epochs):\n",
    "            losses = list()\n",
    "            accuracies = list()\n",
    "            validation = list()\n",
    "            \n",
    "            bar = ProgressBar(max_value=np.floor(len(ytrain)/batch_shape[0]).astype('i'))\n",
    "            minibatch = enumerate(self.get_minibatch(xtrain, ytrain, ultrain, batch_shape=batch_shape))\n",
    "            for i, train_batch in minibatch:\n",
    "                fd = {batch: train_batch['x'], labels: train_batch['y'], K.backend.learning_phase(): 1} #training mode\n",
    "                if (loss_type == 'v_adv'):\n",
    "                    fd.update( {ul_batch: train_batch['ul']} )\n",
    "                \n",
    "                _, acc_val, loss_val = self.sess.run([opt, accuracy, loss], feed_dict=fd)\n",
    "                \n",
    "                accuracies.append( acc_val )\n",
    "                losses.append( loss_val )\n",
    "                bar.update(i)\n",
    "            \n",
    "            #saving accuracies and losses\n",
    "            _accuracies.append( accuracies )\n",
    "            _losses.append(losses)\n",
    "            \n",
    "            log_msg = \"\\nEpoch {} of {} -- average accuracy is {:.3f} (train) -- average loss is {:.3f}\"\n",
    "            print( log_msg.format(epoch+1, epochs, np.asarray(accuracies).mean(), np.asarray(losses).mean()) )\n",
    "            \n",
    "            # validation log\n",
    "            self.validation(xval, yval, batch_shape=batch_shape)\n",
    "            \n",
    "            #saving model\n",
    "            if (save is not None) and (epoch == (epochs-1)):\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(self.sess, save)\n",
    "                print( 'model saved' )\n",
    "        \n",
    "        plt.plot([np.asarray(l).mean() for l in _losses], color='red', linestyle='solid', marker='o', linewidth=2)\n",
    "        plt.plot([np.asarray(a).mean() for a in _accuracies], color='blue', linestyle='solid', marker='o', linewidth=2)\n",
    "        plt.savefig('./train_{}_e{}_m{}_l{}.png'.format(loss_type, epochs, batch_shape[0], batch_shape[1]))\n",
    "        \n",
    "    def test(self, dataset, batch_shape=(64, 400)):\n",
    "        print( 'Test...' )\n",
    "        xtest = np.load( \"{}nltk_xtest.npy\".format(dataset) )\n",
    "        ytest = np.load( \"{}nltk_ytest.npy\".format(dataset) )\n",
    "        ytest = np.reshape(ytest, newshape=(ytest.shape[0], 1))\n",
    "        \n",
    "        labels = tf.placeholder(tf.float32, shape=(None, 1), name='test_labels')\n",
    "        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='test_batch')\n",
    "\n",
    "        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n",
    "        \n",
    "        accuracies = list()\n",
    "        bar = ProgressBar(max_value=np.floor(len(ytest)/batch_shape[0]).astype('i'))\n",
    "        minibatch = enumerate(self.get_minibatch(xtest, ytest, ul=None, batch_shape=batch_shape))\n",
    "        for i, test_batch in minibatch:\n",
    "            fd = {batch: test_batch['x'], labels: test_batch['y'], K.backend.learning_phase(): 0} #test mode\n",
    "            accuracies.append( self.sess.run(accuracy, feed_dict=fd) )\n",
    "            \n",
    "            bar.update(i)\n",
    "        \n",
    "        print( \"\\nAverage accuracy is {:.3f}\".format(np.asarray(accuracies).mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '../dataset/imdb/'\n",
    "n_epochs = 10\n",
    "n_ex = 64\n",
    "ex_len = 400\n",
    "lt = 'none'\n",
    "pm = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "embedding_weights = np.load( \"{}nltk_embedding_matrix.npy\".format(data) )\n",
    "\n",
    "net = Network(session, embedding_weights)\n",
    "net.train(data, batch_shape=(n_ex, ex_len), epochs=n_epochs, loss_type=lt, p_mult=pm, init=None, save=None)\n",
    "net.test(data, batch_shape=(n_ex, ex_len))\n",
    "    \n",
    "K.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bit modification to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, session, dict_weight, dropout=0.3, lstm_units=1024, dense_units=60):\n",
    "        self.sess = session\n",
    "        K.backend.set_session(self.sess)\n",
    "        #defining layers\n",
    "        dict_shape = dict_weight.shape\n",
    "        self.emb = K.layers.Embedding(dict_shape[0], dict_shape[1], weights=[dict_weight], trainable=False, name='embedding')\n",
    "        self.drop = K.layers.Dropout(rate=dropout, seed=91, name='dropout')\n",
    "        self.lstm = K.layers.LSTM(lstm_units, stateful=False, return_sequences=False, name='lstm')\n",
    "        \n",
    "        self.dense = K.layers.Dense(dense_units, activation='relu', name='dense')\n",
    "        self.p = K.layers.Dense(1, activation='sigmoid', name='p')\n",
    "        #defining optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "\n",
    "    def __call__(self, batch, perturbation=None):  \n",
    "        batch1 = batch[:,0:400]\n",
    "        batch2 = batch[:,400:800]\n",
    "        batch3 = batch[:,800:1200]\n",
    "        \n",
    "        embedding1 = self.emb(batch1)\n",
    "        embedding2 = self.emb(batch2)\n",
    "        embedding3 = self.emb(batch3)\n",
    "        \n",
    "        drop1 = self.drop(embedding1)\n",
    "        drop2 = self.drop(embedding2)\n",
    "        drop3 = self.drop(embedding3)\n",
    "            \n",
    "        if (perturbation is not None):\n",
    "            drop1 += perturbation[0]\n",
    "            drop2 += perturbation[1]\n",
    "            drop3 += perturbation[2]\n",
    "        \n",
    "        lstm1 = self.lstm(drop1)\n",
    "        lstm2 = self.lstm(drop2)\n",
    "        lstm3 = self.lstm(drop3)\n",
    "        lstm = tf.concat([lstm1, lstm2, lstm3], axis=1)\n",
    "        dense = self.dense(lstm)\n",
    "        \n",
    "        return self.p(dense), (embedding1, embedding2, embedding3)\n",
    "    \n",
    "    def get_minibatch(self, x, y, ul, batch_shape=(16, 1200)):\n",
    "        x = K.preprocessing.sequence.pad_sequences(x, maxlen=batch_shape[1])\n",
    "        permutations = np.random.permutation( len(y) )\n",
    "        ul_permutations = None\n",
    "        len_ratio = None\n",
    "        if (ul is not None):\n",
    "            ul = K.preprocessing.sequence.pad_sequences(ul, maxlen=batch_shape[1])\n",
    "            ul_permutations = np.random.permutation(len(ul))\n",
    "            len_ratio = len(ul)/len(y)\n",
    "        for s in range(0, len(y), batch_shape[0]):\n",
    "            perm = permutations[s:s+batch_shape[0]]\n",
    "            minibatch = {'x': x[perm], 'y': y[perm]}\n",
    "            if (ul is not None):\n",
    "                ul_perm = ul_permutations[int(np.floor(len_ratio*s)):int(np.floor(len_ratio*(s+batch_shape[0])))]\n",
    "                minibatch.update( {'ul': np.concatenate((ul[ul_perm], x[perm]), axis=0)} )             \n",
    "            yield minibatch\n",
    "    \n",
    "    def get_loss(self, batch, labels):\n",
    "        pred, emb = self(batch)\n",
    "        loss = K.losses.binary_crossentropy(labels, pred)\n",
    "        return tf.reduce_mean( loss ), emb\n",
    "    \n",
    "    def get_adv_loss(self, batch, labels, loss, emb, p_mult):\n",
    "        g1 = tf.gradients(loss, emb[0], aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "        g2 = tf.gradients(loss, emb[1], aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "        g3 = tf.gradients(loss, emb[2], aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "        p_adv = list()\n",
    "        p_adv.append( p_mult * tf.nn.l2_normalize(tf.stop_gradient(g1), dim=1) )\n",
    "        p_adv.append( p_mult * tf.nn.l2_normalize(tf.stop_gradient(g2), dim=1) )\n",
    "        p_adv.append( p_mult * tf.nn.l2_normalize(tf.stop_gradient(g3), dim=1) )\n",
    "        adv_loss = K.losses.binary_crossentropy(labels, self(batch, p_adv)[0])\n",
    "        return tf.reduce_mean( adv_loss )\n",
    "    \n",
    "    def validate(self, xval, yval, batch_shape=(64, 1200), log_path=None):\n",
    "        print( 'Validation...' )\n",
    "        \n",
    "        labels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')\n",
    "        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='batch')\n",
    "\n",
    "        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n",
    "        accuracies = list()\n",
    "        bar = ProgressBar(max_value=np.floor(len(yval)/batch_shape[0]).astype('i'))\n",
    "        minibatch = enumerate(self.get_minibatch(xval, yval, None, batch_shape=batch_shape))\n",
    "        for i, val_batch in minibatch:\n",
    "            fd = {batch: val_batch['x'], labels: val_batch['y'], K.backend.learning_phase(): 0} #test mode\n",
    "            accuracies.append( self.sess.run(accuracy, feed_dict=fd) )\n",
    "            bar.update(i)\n",
    "        \n",
    "        log_msg = \"\\nValidation Average accuracy is {:.3f} -- batch shape {}\"\n",
    "        print( log_msg.format(np.asarray(accuracies).mean(), batch_shape) )\n",
    "        log = None\n",
    "        if(log_path is not None):\n",
    "            log = open(log_path, 'a')\n",
    "            log.write(log_msg.format(np.asarray(accuracies).mean(), batch_shape)+'\\n')\n",
    "            log.close()\n",
    "    \n",
    "    def train(self, dataset, batch_shape=(64, 1200), epochs=10, loss_type='none', p_mult=0.02, save=None, log_path=None):\n",
    "        print( 'Training...' )\n",
    "        xtrain = np.load( \"{}nltk_xtrain.npy\".format(dataset) )\n",
    "        ytrain = np.load( \"{}nltk_ytrain.npy\".format(dataset) )\n",
    "        \n",
    "        xval = list()\n",
    "        yval = list()\n",
    "        for i in range(int(len(ytrain)*0.025)):\n",
    "            xval.append( xtrain[0] ); xval.append( xtrain[-1] )\n",
    "            yval.append( ytrain[0] ); yval.append( ytrain[-1] )\n",
    "            xtrain = np.delete(xtrain, 0); xtrain = np.delete(xtrain, -1)\n",
    "            ytrain = np.delete(ytrain, 0); ytrain = np.delete(ytrain, -1)\n",
    "        xval = np.asarray(xval)\n",
    "        yval = np.asarray(yval)\n",
    "        \n",
    "        yval = np.reshape(yval, newshape=(yval.shape[0],1))\n",
    "        ytrain = np.reshape(ytrain, newshape=(ytrain.shape[0], 1))\n",
    "        \n",
    "        ultrain = np.load( \"{}nltk_ultrain.npy\".format(dataset) ) if (loss_type == 'v_adv') else None \n",
    "        \n",
    "        labels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')\n",
    "        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='batch')\n",
    "        ul_batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='ul_batch')\n",
    "        \n",
    "        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n",
    "        loss, emb = self.get_loss(batch, labels)\n",
    "        if (loss_type == 'adv'):\n",
    "            loss += self.get_adv_loss(batch, labels, loss, emb, p_mult)\n",
    "        elif (loss_type == 'v_adv'):\n",
    "            loss += self.get_v_adv_loss(ul_batch, p_mult)\n",
    "\n",
    "        opt = self.optimizer.minimize( loss )\n",
    "        #initializing parameters\n",
    "        self.sess.run( [var.initializer for var in tf.global_variables() if not('embedding' in var.name)] )\n",
    "                       \n",
    "        _losses = list()\n",
    "        _accuracies = list()\n",
    "        log = None\n",
    "                         \n",
    "        list_ratio = (len(ultrain)/len(ytrain)) if (ultrain is not None) else None\n",
    "        for epoch in range(epochs):\n",
    "            losses = list()\n",
    "            accuracies = list()\n",
    "            \n",
    "            bar = ProgressBar(max_value=np.floor(len(ytrain)/batch_shape[0]).astype('i'))\n",
    "            minibatch = enumerate(self.get_minibatch(xtrain, ytrain, ultrain, batch_shape=batch_shape))\n",
    "            for i, train_batch in minibatch:\n",
    "                fd = {batch: train_batch['x'], labels: train_batch['y'], K.backend.learning_phase(): 1} #training mode\n",
    "                if (loss_type == 'v_adv'):\n",
    "                    fd.update( {ul_batch: train_batch['ul']} )\n",
    "                    \n",
    "                _, acc_val, loss_val = self.sess.run([opt, accuracy, loss], feed_dict=fd)\n",
    "                \n",
    "                accuracies.append( acc_val )\n",
    "                losses.append( loss_val )\n",
    "                bar.update(i)\n",
    "            \n",
    "            _losses.append(losses)\n",
    "            _accuracies.append(accuracies)\n",
    "            \n",
    "            log_msg = \"\\nEpoch {} of {} -- average accuracy is {:.3f} -- average loss is {:.3f}\"\n",
    "            print( log_msg.format(epoch+1, epochs, np.asarray(accuracies).mean(), np.asarray(losses).mean()) )\n",
    "            if(log_path is not None):\n",
    "                log = open(log_path, 'a')\n",
    "                log.write(log_msg.format(epoch+1, epochs, np.asarray(accuracies).mean(), np.asarray(losses).mean())+'\\n')\n",
    "                log.close()\n",
    "        \n",
    "            #validation\n",
    "            self.validate(xval, yval, batch_shape=batch_shape, log_path=log_path)\n",
    "        \n",
    "    def test(self, dataset, batch_shape=(64, 1200), log_path=None):\n",
    "        print( 'Test...' )\n",
    "        xtest = np.load( \"{}nltk_xtest.npy\".format(dataset) )\n",
    "        ytest = np.load( \"{}nltk_ytest.npy\".format(dataset) )\n",
    "        ytest = np.reshape(ytest, newshape=(ytest.shape[0], 1))\n",
    "        \n",
    "        labels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')\n",
    "        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='batch')\n",
    "\n",
    "        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n",
    "        \n",
    "        accuracies = list()\n",
    "        bar = ProgressBar(max_value=np.floor(len(ytest)/batch_shape[0]).astype('i'))\n",
    "        minibatch = enumerate(self.get_minibatch(xtest, ytest, None, batch_shape=batch_shape))\n",
    "        for i, test_batch in minibatch:\n",
    "            fd = {batch: test_batch['x'], labels: test_batch['y'], K.backend.learning_phase(): 0} #test mode\n",
    "            accuracies.append( self.sess.run(accuracy, feed_dict=fd) )\n",
    "            bar.update(i)\n",
    "        \n",
    "        log_msg = \"\\nTest Average accuracy is {:.3f} -- batch shape {}\"\n",
    "        print( log_msg.format(np.asarray(accuracies).mean(), batch_shape) )\n",
    "        \n",
    "        log = None\n",
    "        if(log_path is not None):\n",
    "            log = open(log_path, 'a')\n",
    "            log.write(log_msg.format(np.asarray(accuracies).mean(), batch_shape)+'\\n')\n",
    "            log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '../dataset/imdb/'\n",
    "n_epochs = 10\n",
    "n_ex = 16\n",
    "ex_len = 1200\n",
    "lt = 'none'\n",
    "pm = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "embedding_weights = np.load( \"{}nltk_embedding_matrix.npy\".format(data) )\n",
    "\n",
    "# log text file\n",
    "net_tp = 'baseline' if (lt == 'none') else lt\n",
    "log_path = './pyramidal_{}_bs_{}_ep_{}_sl_{}.txt'.format(net_tp, n_ex, n_epochs, ex_len)\n",
    "log = open(log_path, 'w')\n",
    "log.write('Fancy Network with {} loss, {} epochs, {} batch size, {} maximum string length \\n'.format(net_tp, n_epochs, n_ex, ex_len))\n",
    "log.close()\n",
    "                                  \n",
    "net = Network(session, embedding_weights)\n",
    "net.train(data, batch_shape=(n_ex, ex_len), epochs=n_epochs, loss_type=lt, p_mult=pm, log_path=log_path)\n",
    "net.test(data, batch_shape=(n_ex, ex_len), log_path=log_path)\n",
    "       \n",
    "K.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
